{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Hr17uYPX85vm",
    "outputId": "bb6051ff-e154-4539-f934-be4e3d28b775",
    "tags": []
   },
   "outputs": [],
   "source": [
    "!dpkg --print-architecture\n",
    "\n",
    "# Download ffmpeg\n",
    "\n",
    "#!wget https://johnvansickle.com/ffmpeg/builds/ffmpeg-git-amd64-static.tar.xz\n",
    "#!tar -xf ffmpeg-git-amd64-static.tar.xz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LcmKQgf4Fbw2",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch as T\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "from torch.autograd import Variable\n",
    "from skimage.metrics import structural_similarity\n",
    "import imageio.v3 as iio\n",
    "T.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "71ZpaFgEFwQk",
    "outputId": "59e7e39c-6b4e-4fab-ad2e-e86116582f6c",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torchvision.datasets.utils import download_url\n",
    "\n",
    "download_url(\n",
    "    \"https://github.com/pytorch/vision/blob/main/test/assets/videos/WUzgd7C1pWA.mp4?raw=true\",\n",
    "    \".\",\n",
    "    \"WUzgd7C1pWA.mp4\"\n",
    ")\n",
    "video_path = \"./WUzgd7C1pWA.mp4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vo1dwfHa9zvy",
    "outputId": "01c03960-437e-4abc-93f9-8da0fd03e96a",
    "tags": []
   },
   "outputs": [],
   "source": [
    "!rm output.mp4\n",
    "!ffmpeg -i WUzgd7C1pWA.mp4 -vcodec libx264 -preset ultrafast -crf 35 -acodec aac output.mp4 > /dev/null\n",
    "#!ffmpeg -i WUzgd7C1pWA.mp4 -b:v 2M output.mp4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eUtk7m3p7MVz",
    "outputId": "f2d5c8ea-9415-40fe-bb1e-6fe06df4b2ee",
    "tags": []
   },
   "outputs": [],
   "source": [
    "!./ffmpeg-git-20230621-amd64-static/ffmpeg -i WUzgd7C1pWA.mp4 -i output.mp4 -lavfi libvmaf=\"n_threads=1:n_subsample=10220\" -f null -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IDXqFCeJIP2q",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import subprocess\n",
    "def compare(ImageAPath, ImageBPath):\n",
    "    cmd = \" \".join([\"./ffmpeg-git-20230621-amd64-static/ffmpeg -i\", ImageAPath, \"-i\", ImageBPath, \"-lavfi libvmaf='n_threads=16' -f null -\"])\n",
    "    x = subprocess.getoutput(cmd);\n",
    "    return float(x.split()[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PCH_WHlCIbSB",
    "outputId": "e14395f1-3863-4ed9-abdc-98c77a82596e",
    "tags": []
   },
   "outputs": [],
   "source": [
    "compare(\"./WUzgd7C1pWA.mp4\", \"./output.mp4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "09kv90GuOYA3",
    "outputId": "37da85b0-ceb1-4fd1-e9db-c6da8c11836c",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#RESNET50 can be replaced by something faster and better\n",
    "\n",
    "resnet50 = models.resnet50(pretrained=True)\n",
    "modules=list(resnet50.children())[:-1]\n",
    "resnet50 = nn.Sequential(*modules)\n",
    "\n",
    "# Set to false to stop training image part\n",
    "for p in resnet50.parameters():\n",
    "    p.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Copyright (c) Meta Platforms, Inc. and affiliates.\n",
    "#\n",
    "# This source code is licensed under the MIT license found in the\n",
    "# LICENSE file in the root directory of this source tree.\n",
    "\n",
    "\n",
    "# from https://github.com/toshas/torch_truncnorm\n",
    "\n",
    "import math\n",
    "from numbers import Number\n",
    "\n",
    "import torch\n",
    "from torch.distributions import constraints, Distribution\n",
    "from torch.distributions.utils import broadcast_all\n",
    "\n",
    "CONST_SQRT_2 = math.sqrt(2)\n",
    "CONST_INV_SQRT_2PI = 1 / math.sqrt(2 * math.pi)\n",
    "CONST_INV_SQRT_2 = 1 / math.sqrt(2)\n",
    "CONST_LOG_INV_SQRT_2PI = math.log(CONST_INV_SQRT_2PI)\n",
    "CONST_LOG_SQRT_2PI_E = 0.5 * math.log(2 * math.pi * math.e)\n",
    "\n",
    "\n",
    "class TruncatedStandardNormal(Distribution):\n",
    "    \"\"\"Truncated Standard Normal distribution.\n",
    "    Source: https://people.sc.fsu.edu/~jburkardt/presentations/truncated_normal.pdf\n",
    "    \"\"\"\n",
    "\n",
    "    arg_constraints = {\n",
    "        \"a\": constraints.real,\n",
    "        \"b\": constraints.real,\n",
    "    }\n",
    "    has_rsample = True\n",
    "    eps = 1e-6\n",
    "\n",
    "    def __init__(self, a, b, validate_args=None):\n",
    "        self.a, self.b = broadcast_all(a, b)\n",
    "        if isinstance(a, Number) and isinstance(b, Number):\n",
    "            batch_shape = torch.Size()\n",
    "        else:\n",
    "            batch_shape = self.a.size()\n",
    "        super(TruncatedStandardNormal, self).__init__(\n",
    "            batch_shape, validate_args=validate_args\n",
    "        )\n",
    "        if self.a.dtype != self.b.dtype:\n",
    "            raise ValueError(\"Truncation bounds types are different\")\n",
    "        if any(\n",
    "            (self.a >= self.b)\n",
    "            .view(\n",
    "                -1,\n",
    "            )\n",
    "            .tolist()\n",
    "        ):\n",
    "            raise ValueError(\"Incorrect truncation range\")\n",
    "        eps = self.eps\n",
    "        self._dtype_min_gt_0 = eps\n",
    "        self._dtype_max_lt_1 = 1 - eps\n",
    "        self._little_phi_a = self._little_phi(self.a)\n",
    "        self._little_phi_b = self._little_phi(self.b)\n",
    "        self._big_phi_a = self._big_phi(self.a)\n",
    "        self._big_phi_b = self._big_phi(self.b)\n",
    "        self._Z = (self._big_phi_b - self._big_phi_a).clamp(eps, 1 - eps)\n",
    "        self._log_Z = self._Z.log()\n",
    "        little_phi_coeff_a = torch.nan_to_num(self.a, nan=math.nan)\n",
    "        little_phi_coeff_b = torch.nan_to_num(self.b, nan=math.nan)\n",
    "        self._lpbb_m_lpaa_d_Z = (\n",
    "            self._little_phi_b * little_phi_coeff_b\n",
    "            - self._little_phi_a * little_phi_coeff_a\n",
    "        ) / self._Z\n",
    "        self._mean = -(self._little_phi_b - self._little_phi_a) / self._Z\n",
    "        self._variance = (\n",
    "            1\n",
    "            - self._lpbb_m_lpaa_d_Z\n",
    "            - ((self._little_phi_b - self._little_phi_a) / self._Z) ** 2\n",
    "        )\n",
    "        self._entropy = CONST_LOG_SQRT_2PI_E + self._log_Z - 0.5 * self._lpbb_m_lpaa_d_Z\n",
    "\n",
    "    @constraints.dependent_property\n",
    "    def support(self):\n",
    "        return constraints.interval(self.a, self.b)\n",
    "\n",
    "    @property\n",
    "    def mean(self):\n",
    "        return self._mean\n",
    "\n",
    "    @property\n",
    "    def variance(self):\n",
    "        return self._variance\n",
    "\n",
    "    @property\n",
    "    def entropy(self):\n",
    "        return self._entropy\n",
    "\n",
    "    @property\n",
    "    def auc(self):\n",
    "        return self._Z\n",
    "\n",
    "    @staticmethod\n",
    "    def _little_phi(x):\n",
    "        return (-(x**2) * 0.5).exp() * CONST_INV_SQRT_2PI\n",
    "\n",
    "    def _big_phi(self, x):\n",
    "        phi = 0.5 * (1 + (x * CONST_INV_SQRT_2).erf())\n",
    "        return phi.clamp(self.eps, 1 - self.eps)\n",
    "\n",
    "    @staticmethod\n",
    "    def _inv_big_phi(x):\n",
    "        return CONST_SQRT_2 * (2 * x - 1).erfinv()\n",
    "\n",
    "    def cdf(self, value):\n",
    "        if self._validate_args:\n",
    "            self._validate_sample(value)\n",
    "        return ((self._big_phi(value) - self._big_phi_a) / self._Z).clamp(0, 1)\n",
    "\n",
    "    def icdf(self, value):\n",
    "        y = self._big_phi_a + value * self._Z\n",
    "        y = y.clamp(self.eps, 1 - self.eps)\n",
    "        return self._inv_big_phi(y)\n",
    "\n",
    "    def log_prob(self, value):\n",
    "        if self._validate_args:\n",
    "            self._validate_sample(value)\n",
    "        return CONST_LOG_INV_SQRT_2PI - self._log_Z - (value**2) * 0.5\n",
    "\n",
    "    def rsample(self, sample_shape=None):\n",
    "        if sample_shape is None:\n",
    "            sample_shape = torch.Size([])\n",
    "        shape = self._extended_shape(sample_shape)\n",
    "        p = torch.empty(shape, device=self.a.device).uniform_(\n",
    "            self._dtype_min_gt_0, self._dtype_max_lt_1\n",
    "        )\n",
    "        return self.icdf(p)\n",
    "\n",
    "\n",
    "class TruncatedNormal(TruncatedStandardNormal):\n",
    "    \"\"\"Truncated Normal distribution.\n",
    "    https://people.sc.fsu.edu/~jburkardt/presentations/truncated_normal.pdf\n",
    "    \"\"\"\n",
    "\n",
    "    has_rsample = True\n",
    "\n",
    "    def __init__(self, loc, scale, a, b, validate_args=None):\n",
    "        scale = scale.clamp_min(self.eps)\n",
    "        self.loc, self.scale, a, b = broadcast_all(loc, scale, a, b)\n",
    "        self._non_std_a = a\n",
    "        self._non_std_b = b\n",
    "        a = (a - self.loc) / self.scale\n",
    "        b = (b - self.loc) / self.scale\n",
    "        super(TruncatedNormal, self).__init__(a, b, validate_args=validate_args)\n",
    "        self._log_scale = self.scale.log()\n",
    "        self._mean = self._mean * self.scale + self.loc\n",
    "        self._variance = self._variance * self.scale**2\n",
    "        self._entropy += self._log_scale\n",
    "\n",
    "    def _to_std_rv(self, value):\n",
    "        return (value - self.loc) / self.scale\n",
    "\n",
    "    def _from_std_rv(self, value):\n",
    "        return value * self.scale + self.loc\n",
    "\n",
    "    def cdf(self, value):\n",
    "        return super(TruncatedNormal, self).cdf(self._to_std_rv(value))\n",
    "\n",
    "    def icdf(self, value):\n",
    "        sample = self._from_std_rv(super().icdf(value))\n",
    "\n",
    "        # clamp data but keep gradients\n",
    "        sample_clip = torch.stack(\n",
    "            [sample.detach(), self._non_std_a.detach().expand_as(sample)], 0\n",
    "        ).max(0)[0]\n",
    "        sample_clip = torch.stack(\n",
    "            [sample_clip, self._non_std_b.detach().expand_as(sample)], 0\n",
    "        ).min(0)[0]\n",
    "        sample.data.copy_(sample_clip)\n",
    "        return sample\n",
    "\n",
    "    def log_prob(self, value):\n",
    "        value = self._to_std_rv(value)\n",
    "        return super(TruncatedNormal, self).log_prob(value) - self._log_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "L80JDPWFIgSa",
    "outputId": "98aa6e12-42a7-41a2-9c98-f929b68ee7c2",
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install ffmpeg-bitrate-stats\n",
    "from ffmpeg_bitrate_stats import BitrateStats\n",
    "\n",
    "import os\n",
    "import gym\n",
    "from gym import spaces\n",
    "import numpy as np\n",
    "\n",
    "import subprocess\n",
    "\n",
    "game_steps = 5\n",
    "\n",
    "import time\n",
    "\n",
    "class VideoEncodingEnvironment(gym.Env):\n",
    "    def __init__(self, target_video):\n",
    "        super(VideoEncodingEnvironment, self).__init__()\n",
    "\n",
    "        self.target_video = target_video  # Target video representation\n",
    "        #self.max_steps = max_steps  # Maximum number of steps for encoding\n",
    "\n",
    "        self.current_step = 0  # Current step in the encoding process\n",
    "        \n",
    "        self.size = os.stat(target_video).st_size  # Current video representation\n",
    "        self.initial_size = self.size\n",
    "        \n",
    "        ffbs = BitrateStats(target_video)\n",
    "        ffbs.calculate_statistics()\n",
    "\n",
    "        self.create_embedding()\n",
    "\n",
    "        self.bitrate = ffbs.avg_bitrate\n",
    "\n",
    "        self.quality = 100\n",
    "        \n",
    "        self.initial_bitrate = 0\n",
    "        \n",
    "        self.prev_score = 0\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "    def step(self, action):\n",
    "        # Update step count\n",
    "        self.current_step += 1\n",
    "        \n",
    "        action = action * self.initial_bitrate * 1000\n",
    "\n",
    "        # Calculate quality\n",
    "        os.remove(\"./output.mp4\") if os.path.exists(\"./output.mp4\") else None\n",
    "        cmd = \" \".join([\"ffmpeg -i\", self.target_video, \"-vcodec libx264\",\"-b:v\", str(int(action)), \"./output.mp4\", \"-hide_banner\", \"-preset ultrafast\", \"-loglevel error\"])\n",
    "        x = subprocess.run(cmd, shell = True, stdout = subprocess.DEVNULL, stderr=subprocess.PIPE)\n",
    "        if x.stderr.decode() != \"\":\n",
    "            print(\"Error\")\n",
    "            self.quality = 0\n",
    "            state = np.array([[self.current_step, self.initial_bitrate, self. initial_size, self.bitrate, self.quality, self.size]])\n",
    "            state = np.concatenate((state, self.embed.reshape((1,-1)).detach().numpy()), axis = 1).reshape(-1)\n",
    "            return state, -1000, True, \"\"\n",
    "\n",
    "        ffbs = BitrateStats(\"./output.mp4\")\n",
    "        ffbs.calculate_statistics()\n",
    "        self.bitrate = ffbs.avg_bitrate\n",
    "\n",
    "        self.quality = compare(self.target_video, './output.mp4')\n",
    "\n",
    "        # Update size\n",
    "        self.size = os.stat(\"./output.mp4\").st_size\n",
    "\n",
    "        state = np.array([[self.current_step, self.initial_bitrate, self. initial_size, self.bitrate, self.quality, self.size]])\n",
    "        state = np.concatenate((state, self.embed.reshape((1,-1)).detach().numpy()), axis = 1).reshape(-1)\n",
    "        \n",
    "        \n",
    "        target_reached = self.quality > 84 and self.quality < 86\n",
    "                \n",
    "        # Reward\n",
    "\n",
    "        score = 225 - ((self.quality - 85)**2) \n",
    "        \n",
    "        if self.quality > 92:\n",
    "            score -= (self.quality - 92) * 12.5\n",
    "        \n",
    "        if self.bitrate > self.initial_bitrate:\n",
    "            print(\"WTF too high\")\n",
    "            self.quality = 0\n",
    "            state = np.array([[self.current_step, self.initial_bitrate, self. initial_size, self.bitrate, self.quality, self.size]])\n",
    "            state = np.concatenate((state, self.embed.reshape((1,-1)).detach().numpy()), axis = 1).reshape(-1)\n",
    "            return state, -5000, True, \"\"\n",
    "            \n",
    "        \n",
    "        #Give bonus for reaching target\n",
    "        if target_reached:\n",
    "            score = score + (200 * (game_steps + 1 - self.current_step)) \n",
    "        \n",
    "        reward = score - self.prev_score - ((abs(self.quality - 85)/1.5) ** 2 ) - 10\n",
    "        if self.current_step == 1:\n",
    "            reward -= 80\n",
    "        self.prev_score = score\n",
    "        \n",
    "        return state, reward, self.current_step > (game_steps - 1) or target_reached, \"\"\n",
    "\n",
    "    def create_embedding(self):\n",
    "        #Create embedding for a single image\n",
    "        from PIL import Image\n",
    "        #Extract the first frame\n",
    "        #Can be possible to do more efficiently without iio.imiter\n",
    "        iio.imwrite(\"extracted.jpg\", list(iio.imiter(self.target_video))[0])\n",
    "        import torchvision.transforms as transforms\n",
    "        transform = transforms.Compose([transforms.PILToTensor()])\n",
    "        img = Image.open(\"extracted.jpg\")\n",
    "        tensor = transform(img)\n",
    "        tensor = tensor.unsqueeze(0).float()\n",
    "        self.embed = resnet50(tensor).reshape(-1)\n",
    "\n",
    "    def reset(self, target_video):\n",
    "        self.target_video = target_video\n",
    "        self.current_step = 0  # Current step in the encoding process\n",
    "        self.prev_score = 0\n",
    "        self.reward = 0\n",
    "        self.size = os.stat(self.target_video).st_size  # Current video representation\n",
    "        self.initial_size = self.size\n",
    "\n",
    "\n",
    "        ffbs = BitrateStats(self.target_video)\n",
    "        ffbs.calculate_statistics()\n",
    "        self.bitrate = ffbs.avg_bitrate\n",
    "        \n",
    "        self.initial_bitrate = self.bitrate\n",
    "\n",
    "\n",
    "        self.quality = 100\n",
    "\n",
    "        self.create_embedding()\n",
    "        state = np.array([[self.current_step, self.initial_bitrate, self. initial_size, self.bitrate, self.quality, self.size]])\n",
    "        state = np.concatenate((state, self.embed.reshape((1,-1)).detach().numpy()), axis = 1).reshape(-1)\n",
    "\n",
    "        return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZYMHfAMkQTql",
    "outputId": "21d49f84-75de-4ad1-c82a-5c05807854b5",
    "tags": []
   },
   "outputs": [],
   "source": [
    "%mkdir ppo\n",
    "import os\n",
    "import numpy as np\n",
    "import torch as T\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions.normal import Normal\n",
    "from torch.distributions.beta import Beta\n",
    "\n",
    "\n",
    "class PPOMemory:\n",
    "    def __init__(self, batch_size):\n",
    "        self.states = []\n",
    "        self.probs = []\n",
    "        self.vals = []\n",
    "        self.actions = []\n",
    "        self.rewards = []\n",
    "        self.dones = []\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def generate_batches(self):\n",
    "        n_states = len(self.states)\n",
    "        batch_start = np.arange(0, n_states, self.batch_size)\n",
    "        indices = np.arange(n_states, dtype=np.int64)\n",
    "        np.random.shuffle(indices)\n",
    "        batches = [indices[i:i+self.batch_size] for i in batch_start]\n",
    "\n",
    "        return np.array(self.states),\\\n",
    "                np.array(self.actions),\\\n",
    "                np.array(self.probs),\\\n",
    "                np.array(self.vals),\\\n",
    "                np.array(self.rewards),\\\n",
    "                np.array(self.dones),\\\n",
    "                batches\n",
    "\n",
    "    def store_memory(self, state, action, probs, vals, reward, done):\n",
    "        self.states.append(state)\n",
    "        self.actions.append(action)\n",
    "        self.probs.append(probs)\n",
    "        self.vals.append(vals)\n",
    "        self.rewards.append(reward)\n",
    "        self.dones.append(done)\n",
    "\n",
    "    def clear_memory(self):\n",
    "        self.states = []\n",
    "        self.probs = []\n",
    "        self.actions = []\n",
    "        self.rewards = []\n",
    "        self.dones = []\n",
    "        self.vals = []\n",
    "\n",
    "class ActorNetwork(nn.Module):\n",
    "    def __init__(self, n_actions, input_dims, alpha,\n",
    "            fc1_dims=256, fc2_dims=256, chkpt_dir='ppo'):\n",
    "        super(ActorNetwork, self).__init__()\n",
    "\n",
    "        self.checkpoint_file = os.path.join(chkpt_dir, 'actor_torch_ppo')\n",
    "        self.actor = nn.Sequential(\n",
    "                nn.Linear(input_dims, fc1_dims),\n",
    "                nn.Sigmoid(),\n",
    "                nn.Linear(fc1_dims, fc2_dims),\n",
    "                nn.Sigmoid(),\n",
    "                nn.Linear(fc1_dims, fc2_dims),\n",
    "                nn.Sigmoid(),\n",
    "                nn.Linear(fc2_dims, 2)\n",
    "        )\n",
    "\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=alpha)\n",
    "        self.device = T.device('cuda:0' if T.cuda.is_available() else 'cpu')\n",
    "        self.to(self.device)\n",
    "\n",
    "    def forward(self, state):\n",
    "        dist = self.actor(state)\n",
    "        dist = torch.abs(dist).reshape(-1,2)\n",
    "        print(\"loc and scale\", dist)\n",
    "        dist = TruncatedNormal(dist[:,0] , dist[:,1] , 0, 1)\n",
    "        return dist\n",
    "\n",
    "    def save_checkpoint(self):\n",
    "        T.save(self.state_dict(), self.checkpoint_file)\n",
    "\n",
    "    def load_checkpoint(self):\n",
    "        self.load_state_dict(T.load(self.checkpoint_file))\n",
    "\n",
    "class CriticNetwork(nn.Module):\n",
    "    def __init__(self, input_dims, alpha, fc1_dims=256, fc2_dims=256,\n",
    "            chkpt_dir='ppo'):\n",
    "        super(CriticNetwork, self).__init__()\n",
    "\n",
    "        self.checkpoint_file = os.path.join(chkpt_dir, 'critic_torch_ppo')\n",
    "        self.critic = nn.Sequential(\n",
    "                nn.Linear(input_dims, fc1_dims),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(fc1_dims, fc2_dims),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(fc2_dims, 1)\n",
    "        )\n",
    "\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=alpha)\n",
    "        self.device = T.device('cuda:0' if T.cuda.is_available() else 'cpu')\n",
    "        self.to(self.device)\n",
    "\n",
    "    def forward(self, state):\n",
    "        value = self.critic(state)\n",
    "\n",
    "        return value\n",
    "\n",
    "    def save_checkpoint(self):\n",
    "        T.save(self.state_dict(), self.checkpoint_file)\n",
    "\n",
    "    def load_checkpoint(self):\n",
    "        self.load_state_dict(T.load(self.checkpoint_file))\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self, n_actions, input_dims, gamma=0.99, alpha=0.0003, gae_lambda=0.95,\n",
    "            policy_clip=0.1, batch_size=64, n_epochs=10):\n",
    "        self.gamma = gamma\n",
    "        self.policy_clip = policy_clip\n",
    "        self.n_epochs = n_epochs\n",
    "        self.gae_lambda = gae_lambda\n",
    "\n",
    "        self.actor = ActorNetwork(n_actions, input_dims, alpha)\n",
    "        self.critic = CriticNetwork(input_dims, alpha)\n",
    "        self.memory = PPOMemory(batch_size)\n",
    "\n",
    "    def remember(self, state, action, probs, vals, reward, done):\n",
    "        self.memory.store_memory(state, action, probs, vals, reward, done)\n",
    "\n",
    "    def save_models(self):\n",
    "        print('... saving models ...')\n",
    "        self.actor.save_checkpoint()\n",
    "        self.critic.save_checkpoint()\n",
    "\n",
    "    def load_models(self):\n",
    "        print('... loading models ...')\n",
    "        self.actor.load_checkpoint()\n",
    "        self.critic.load_checkpoint()\n",
    "\n",
    "    def choose_action(self, observation):\n",
    "        state = T.tensor(observation, dtype=T.float).to(self.actor.device)\n",
    "\n",
    "        dist = self.actor(state)\n",
    "\n",
    "        value = self.critic(state)\n",
    "        #print(\"Distribution\", dist, value)\n",
    "        action = dist.sample()\n",
    "\n",
    "        probs = T.squeeze(dist.log_prob(action)).item()\n",
    "        action = T.squeeze(action).item()\n",
    "        value = T.squeeze(value).item()\n",
    "\n",
    "        return action, probs, value\n",
    "\n",
    "    def learn(self):\n",
    "        for _ in range(self.n_epochs):\n",
    "            state_arr, action_arr, old_prob_arr, vals_arr,\\\n",
    "            reward_arr, dones_arr, batches = \\\n",
    "                    self.memory.generate_batches()\n",
    "\n",
    "            values = vals_arr\n",
    "            advantage = np.zeros(len(reward_arr), dtype=np.float32)\n",
    "\n",
    "            for t in range(len(reward_arr)-1):\n",
    "                discount = 1\n",
    "                a_t = 0\n",
    "                for k in range(t, len(reward_arr)-1):\n",
    "                    a_t += discount*(reward_arr[k] + self.gamma*values[k+1]*\\\n",
    "                            (1-int(dones_arr[k])) - values[k])\n",
    "                    discount *= self.gamma*self.gae_lambda\n",
    "                advantage[t] = a_t\n",
    "            advantage = T.tensor(advantage).to(self.actor.device)\n",
    "\n",
    "            values = T.tensor(values).to(self.actor.device)\n",
    "            for batch in batches:\n",
    "                states = T.tensor(state_arr[batch], dtype=T.float).to(self.actor.device)\n",
    "                old_probs = T.tensor(old_prob_arr[batch]).to(self.actor.device)\n",
    "                actions = T.tensor(action_arr[batch]).to(self.actor.device)\n",
    "\n",
    "                dist = self.actor(states)\n",
    "                critic_value = self.critic(states)\n",
    "\n",
    "                critic_value = T.squeeze(critic_value)\n",
    "\n",
    "                new_probs = dist.log_prob(actions)\n",
    "                prob_ratio = new_probs.exp() / old_probs.exp()\n",
    "                #prob_ratio = (new_probs - old_probs).exp()\n",
    "                weighted_probs = advantage[batch] * prob_ratio\n",
    "                weighted_clipped_probs = T.clamp(prob_ratio, 1-self.policy_clip,\n",
    "                        1+self.policy_clip)*advantage[batch]\n",
    "                actor_loss = -T.min(weighted_probs, weighted_clipped_probs).mean()\n",
    "\n",
    "                returns = advantage[batch] + values[batch]\n",
    "                critic_loss = (returns-critic_value)**2\n",
    "                critic_loss = critic_loss.mean()\n",
    "\n",
    "                total_loss = actor_loss + 0.5*critic_loss\n",
    "                self.actor.optimizer.zero_grad()\n",
    "                self.critic.optimizer.zero_grad()\n",
    "                total_loss.backward()\n",
    "                self.actor.optimizer.step()\n",
    "                self.critic.optimizer.step()\n",
    "\n",
    "        self.memory.clear_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1EWvoHT1VXlv",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_learning_curve(x, scores, figure_file):\n",
    "    running_avg = np.zeros(len(scores))\n",
    "    for i in range(len(running_avg)):\n",
    "        running_avg[i] = np.mean(scores[max(0, i-100):(i+1)])\n",
    "    plt.plot(x, running_avg)\n",
    "    plt.title('Running average of previous 100 scores')\n",
    "    plt.savefig(figure_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "oc-ibKzWVilR",
    "outputId": "a86d5d0c-9882-4d6f-90c3-576509cc96b8",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import gym\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "env = VideoEncodingEnvironment(\"./WUzgd7C1pWA.mp4\")#gym.make('CartPole-v0')\n",
    "N = 100\n",
    "batch_size = 256\n",
    "n_epochs = 20\n",
    "alpha = 0.0003\n",
    "agent = Agent(n_actions=1, batch_size=batch_size,\n",
    "                alpha=alpha, n_epochs=n_epochs,\n",
    "                input_dims=(2054))\n",
    "#agent.load_models()\n",
    "\n",
    "figure_file = 'cartpole.png'\n",
    "\n",
    "best_score = -1000\n",
    "score_history = [] \n",
    "\n",
    "learn_iters = 0\n",
    "avg_score = 0\n",
    "n_steps = 0\n",
    "\n",
    "\n",
    "data_dir = \"data/\"\n",
    "\n",
    "videos = os.listdir(data_dir)\n",
    "random.Random(0).shuffle(videos)\n",
    "\n",
    "starting_percentage = 0.25\n",
    "\n",
    "for i in tqdm(videos[:1000]):\n",
    "    observation = env.reset( data_dir + i)\n",
    "    observation, _, done, _ = env.step(starting_percentage)\n",
    "    \n",
    "    print(\"Target video:\", i, \"Initial bitrate:\", \"{:.2f}\".format(env.initial_bitrate), \"\\nChosen bitrate percentage:\", starting_percentage, \"Quality:\", \"{:.2f}\".format(env.quality))\n",
    "    if done:\n",
    "        continue\n",
    "    done = False\n",
    "    score = 0\n",
    "    while not done:\n",
    "        action, prob, val = agent.choose_action(observation)\n",
    "        observation_, reward, done, info = env.step(action)\n",
    "        print(\"Chosen bitrate percentage:\", \"{:.2f}\".format(action), \"Reward:\", \"{:.2f}\".format(reward), \"Quality:\", \"{:.2f}\".format(env.quality))\n",
    "        n_steps += 1\n",
    "        score += reward\n",
    "        agent.remember(observation, action, prob, val, reward, done)\n",
    "        if n_steps % N == 0:\n",
    "            print(\"Learning\")\n",
    "            avg_score = np.mean(score_history[-100:])\n",
    "            if avg_score > best_score:\n",
    "                best_score = avg_score\n",
    "                agent.save_models()\n",
    "            agent.learn()\n",
    "            learn_iters += 1\n",
    "        observation = observation_\n",
    "    score_history.append(score)\n",
    "\n",
    "    print('episode', i, 'score %.1f' % score, 'avg score %.1f' % avg_score,\n",
    "            'time_steps', n_steps, 'learning_steps', learn_iters)\n",
    "x = [i+1 for i in range(len(score_history))]\n",
    "plot_learning_curve(x, score_history, figure_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
